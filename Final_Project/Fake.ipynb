{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28297f24",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-telegram-bot in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (13.12)\n",
      "Requirement already satisfied: APScheduler==3.6.3 in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from python-telegram-bot) (3.6.3)\n",
      "Requirement already satisfied: cachetools==4.2.2 in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from python-telegram-bot) (4.2.2)\n",
      "Requirement already satisfied: pytz>=2018.6 in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from python-telegram-bot) (2021.3)\n",
      "Requirement already satisfied: certifi in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from python-telegram-bot) (2021.10.8)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from python-telegram-bot) (6.1)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from APScheduler==3.6.3->python-telegram-bot) (1.16.0)\n",
      "Requirement already satisfied: setuptools>=0.7 in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from APScheduler==3.6.3->python-telegram-bot) (58.0.4)\n",
      "Requirement already satisfied: tzlocal>=1.2 in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from APScheduler==3.6.3->python-telegram-bot) (4.2)\n",
      "Requirement already satisfied: tzdata in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from tzlocal>=1.2->APScheduler==3.6.3->python-telegram-bot) (2022.1)\n",
      "Requirement already satisfied: pytz-deprecation-shim in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from tzlocal>=1.2->APScheduler==3.6.3->python-telegram-bot) (0.1.0.post0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install python-telegram-bot --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da8960b3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_addons\n",
      "  Downloading tensorflow_addons-0.17.1-cp39-cp39-win_amd64.whl (758 kB)\n",
      "     ------------------------------------- 758.1/758.1 kB 12.1 MB/s eta 0:00:00\n",
      "Collecting typeguard>=2.7\n",
      "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from tensorflow_addons) (21.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from packaging->tensorflow_addons) (3.0.4)\n",
      "Installing collected packages: typeguard, tensorflow_addons\n",
      "Successfully installed tensorflow_addons-0.17.1 typeguard-2.13.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f25f0170",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting telegram\n",
      "  Downloading telegram-0.0.1.tar.gz (879 bytes)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: telegram\n",
      "  Building wheel for telegram (setup.py): started\n",
      "  Building wheel for telegram (setup.py): finished with status 'done'\n",
      "  Created wheel for telegram: filename=telegram-0.0.1-py3-none-any.whl size=1307 sha256=24bf57f09e13cef7bbd011a73393b96eb4d209d4f9df5cd2c9c1b77b425a2181\n",
      "  Stored in directory: c:\\users\\bitcamp\\appdata\\local\\pip\\cache\\wheels\\11\\7a\\5d\\62391dcb6b9a45247192c3a711bf03ed513c14218a8b275a63\n",
      "Successfully built telegram\n",
      "Installing collected packages: telegram\n",
      "Successfully installed telegram-0.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "# !pip install telegram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f2c3237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc6db620",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import font_manager, rc\n",
    "font_path = \"C:/Windows/Fonts/malgun.ttf\"\n",
    "font = font_manager.FontProperties(fname=font_path).get_name()\n",
    "rc('font', family=font)\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77b9b31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import telegram\n",
    "from telegram.ext import Updater\n",
    "from telegram.ext import MessageHandler, Filters\n",
    "from telegram import chat\n",
    "# from bob_telegram_tools.bot import TelegramBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8532711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a3ca2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import urllib.request\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de6bd050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e8c260f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import MySQLdb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93cc8152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q_TKN = \"<usr>\"\n",
    "# A_TKN = \"<sys>\"\n",
    "# BOS = '</s>'\n",
    "# EOS = '</s>'\n",
    "# MASK = '<unused0>'\n",
    "# SENT = '<unused1>'\n",
    "# PAD = '<pad>'\n",
    "\n",
    "# model1 = torch.load('./data/GPT2_model/gpt_finance.pth')\n",
    "# koGPT2_TOKENIZER = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
    "#             bos_token=BOS, eos_token=EOS, unk_token='<unk>',\n",
    "#             pad_token=PAD, mask_token=MASK) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f029891b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kospi_list = pd.read_csv('./data/recent_kospi_list.csv')\n",
    "# corp_list = kospi_list['Name']\n",
    "news_lst1 = ['최신뉴스','최근뉴스']\n",
    "news_lst2 = ['긍정뉴스', '호재', '좋은소식','호재는']\n",
    "news_lst3 = ['부정뉴스', '악재','악재는?']\n",
    "news_lst4 = ['키워드','관련있어','관련있어?']\n",
    "\n",
    "\n",
    "reco_lst1 = ['살만한 주식 뭐 있어?','살만한 종목 뭐 있어?','지금 살만한 종목이야 뭐야?','투자 추천 좀 해줘봐']\n",
    "reco_lst2 = ['어떤 업종이 괜찮아?','괜찮은 업종이 어떨꺼 같아?','오르는 업종이 어떤거야?']\n",
    "reco_lst3 = ['코스피가 어떻게 될꺼 같아?','코스피 앞으로 어떻게 될까?','주식시장 앞으로 어떻게 될것 같아?']\n",
    "reco_lst4 = ['시가','종가','영업이익','PER']\n",
    "reco_lst5 = ['삼성전자 어떨꺼 같아?','삼성전자 어때 보여?','삼성전자 오를까? 내릴까?']\n",
    "\n",
    " ######## 함수 ########\n",
    "\n",
    "# def crawl_news(corp,page=1,num=5,bgn_date='2022.03.01',end_date='2022.03.30'):\n",
    "    \n",
    "#     bgn_date1 = bgn_date\n",
    "#     bgn_date2 = bgn_date.replace('.','')\n",
    "#     end_date1 = end_date\n",
    "#     end_date2 = end_date.replace('.','')\n",
    "    \n",
    "#     title_lst = []\n",
    "#     url_lst = []\n",
    "#     date_lst = []\n",
    "\n",
    "#     for pg in range(1,page+1):\n",
    "\n",
    "#         page_num = pg *10 - 9\n",
    "\n",
    "#         url = f'https://search.naver.com/search.naver?where=news&sm=tab_pge&query={corp}&sort=0&photo=0&field=0&pd=3&ds={bgn_date1}&de={end_date1}&cluster_rank=24&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:r,p:from{bgn_date2}to{end_date2},a:all&start={page_num}'\n",
    "#         res = requests.get(url)\n",
    "#         soup = BeautifulSoup(res.text , 'html.parser')\n",
    "#         lis = soup.select('#main_pack > section > div > div.group_news > ul>li')\n",
    "\n",
    "#         for li in lis:\n",
    "#             #제목\n",
    "#             title = li.select('div.news_wrap.api_ani_send > div > a')[0].text\n",
    "\n",
    "#             title_lst.append(title)\n",
    "\n",
    "#             # url\n",
    "#             url_path = li.select('div.news_wrap.api_ani_send > div > a')[0]['href']\n",
    "#             url_lst.append(url_path)\n",
    "\n",
    "#             #날짜\n",
    "\n",
    "#             if len(li.select('div.news_info > div.info_group > span'))==1:\n",
    "#                 date = li.select('div.news_info > div.info_group > span')[0].text\n",
    "#                 date_lst.append(date)\n",
    "\n",
    "\n",
    "#             if len(li.select('div.news_info > div.info_group > span'))==2:\n",
    "#                 date = li.select('div.news_info > div.info_group > span')[1].text\n",
    "#                 date_lst.append(date)\n",
    "    \n",
    "#     df = pd.DataFrame({'날짜':date_lst,'뉴스제목':title_lst,'url':url_lst})\n",
    "    \n",
    "#     output_result = ''\n",
    "#     for i in range(len(df)):\n",
    "#         title = df['뉴스제목'].iloc[i]\n",
    "#         news_url = df['url'].iloc[i]\n",
    "#         output_result += title + \"\\n\" + news_url + \"\\n\\n\"\n",
    "#         if i == num:\n",
    "#             break\n",
    "        \n",
    "#     return df, output_result\n",
    "\n",
    "# def finance_gpt(user_text,tokenizer, model):\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         answer = \"\"\n",
    "#         while 1:\n",
    "#             input_ids = torch.LongTensor(tokenizer.encode(Q_TKN + user_text + SENT + A_TKN + answer)).unsqueeze(dim=0)\n",
    "#             pred = model(input_ids)\n",
    "#             pred = pred.logits\n",
    "#             gen = tokenizer.convert_ids_to_tokens(torch.argmax(pred, dim=-1).squeeze().numpy().tolist())[-1]\n",
    "#             if gen == EOS:\n",
    "#                 break\n",
    "#             answer += gen.replace(\"▁\", \" \")\n",
    "#         answer = answer.strip()\n",
    "#     return answer\n",
    "\n",
    "# # 한글,영어만 남기기\n",
    "# def clean_text(docs):\n",
    "#     docs = re.sub('[^가-힣A-Za-z ]', '', str(docs))\n",
    "#     docs = re.sub('\\s+', ' ', docs)\n",
    "#     docs = '' if docs== ' ' else docs\n",
    "#     return docs\n",
    "\n",
    "# # 명사 추출\n",
    "# def han_noun(docs):\n",
    "#     han = Hannanum()\n",
    "#     docs = han.nouns(docs)\n",
    "#     return docs\n",
    "\n",
    "# # 불용어 제거+ 한글자 이상만 남기기\n",
    "# stw_list = pd.read_csv('./data/stopwords-ko.txt')\n",
    "# def remove_stwords(docs):\n",
    "#     docs = [w for w in docs if not w in stw_list]\n",
    "#     docs = '' if docs== ' ' else docs\n",
    "#     docs = [w for w in docs if len(w)>1]\n",
    "#     return docs\n",
    "\n",
    "# # 크롤링 뉴스 키워드 선출\n",
    "# def kw_crawl_news(corp,bgn_date='2022-03-01',end_date='2022-03-30'):\n",
    "#     df_n, _ = crawl_news(corp,page=10)\n",
    "#     df_n = df_n.set_index('날짜')\n",
    "#     df_n.index = pd.to_datetime(df_n.index)\n",
    "#     df_n['뉴스'] = df_n['뉴스제목'].apply(clean_text)\n",
    "#     df_n['뉴스'] = df_n['뉴스'].apply(han_noun)\n",
    "#     df_n['뉴스'] =  df_n['뉴스'].apply(remove_stwords)\n",
    "#     df_stw = pd.read_csv('./data/뉴스불용어2.csv',index_col=1)\n",
    "#     stw_lst = df_stw['불용어'].tolist()\n",
    "#     kw_dict = dict()\n",
    "\n",
    "#     token_lst = df_n['뉴스'].loc[bgn_date:end_date]\n",
    "            \n",
    "#     for tokens in token_lst:\n",
    "#         for word in tokens:\n",
    "                \n",
    "#             if not word in kw_dict.keys():\n",
    "#                 kw_dict[word] = 1\n",
    "#             else:\n",
    "#                 kw_dict[word] += 1\n",
    "\n",
    "#     kw_dict = dict(sorted(kw_dict.items(), key = lambda x: x[1],reverse=True))\n",
    "#     keys = pd.Series(kw_dict.keys()).tolist()\n",
    "#     for key in keys:\n",
    "#         if (key in stw_lst) | (key == corp):\n",
    "#             del kw_dict[key]\n",
    "            \n",
    "#     return kw_dict\n",
    "\n",
    "# # 키워드 시각화\n",
    "# def show_kw(kw_dict,num=10):\n",
    "#     plt.figure(figsize=(10,8))\n",
    "#     plt.rc('ytick', labelsize=15)  # y축 눈금 폰트 크기\n",
    "#     plt.title('키워드 상위 15')\n",
    "#     keyword_list = list(kw_dict.keys())[:num]\n",
    "#     keyword_list.reverse()\n",
    "#     keyword_count = list(kw_dict.values())[:num]\n",
    "#     keyword_count.reverse()\n",
    "#     sns.barplot(x=keyword_count, y=keyword_list)\n",
    "#     plt.gca().invert_yaxis()\n",
    "#     return plt\n",
    "\n",
    "# # 최고 성능의 모델 불러오기\n",
    "# def call_senti_model():\n",
    "#     BEST_MODEL_NAME = './data/model/best_model.h5'\n",
    "#     sentiment_model_best = tf.keras.models.load_model(BEST_MODEL_NAME,\n",
    "#                                                       custom_objects={'TFBertForSequenceClassification': TFBertForSequenceClassification})\n",
    "#     MODEL_NAME = \"klue/bert-base\"\n",
    "#     model = TFBertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3, from_pt=True)\n",
    "#     tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "#     return sentiment_model_best, tokenizer\n",
    "\n",
    "# # 데이터형태 맞춰주기\n",
    "# def senti_text_data(X_data,tokenizer,MAX_SEQ_LEN=64):\n",
    "#     # BERT 입력으로 들어가는 token, mask, segment, target 저장용 리스트\n",
    "#     tokens, masks, segments, targets = [], [], [], []\n",
    "    \n",
    "#     for X in tqdm(X_data):\n",
    "#         # token: 입력 문장 토큰화\n",
    "#         token = tokenizer.encode(X, truncation = True, padding = 'max_length', max_length = MAX_SEQ_LEN)\n",
    "        \n",
    "#         # Mask: 토큰화한 문장 내 패딩이 아닌 경우 1, 패딩인 경우 0으로 초기화\n",
    "#         num_zeros = token.count(0)\n",
    "#         mask = [1] * (MAX_SEQ_LEN - num_zeros) + [0] * num_zeros\n",
    "        \n",
    "#         # segment: 문장 전후관계 구분: 오직 한 문장이므로 모두 0으로 초기화\n",
    "#         segment = [0]*MAX_SEQ_LEN\n",
    "\n",
    "#         tokens.append(token)\n",
    "#         masks.append(mask)\n",
    "#         segments.append(segment)\n",
    "\n",
    "\n",
    "#     # numpy array로 저장\n",
    "#     tokens = np.array(tokens)\n",
    "#     masks = np.array(masks)\n",
    "#     segments = np.array(segments)\n",
    "#     return [tokens, masks, segments]\n",
    "\n",
    "# # BERT 긍부정 판별\n",
    "# def bert_clsfy_news(corp,senti_model,tokenizer,senti=1,page=5,num=5): #1 2 0 긍정 부정 중립\n",
    "#     df_n,_ = crawl_news(corp,page)\n",
    "#     df_n = df_n.set_index('날짜')\n",
    "#     df_n.index = pd.to_datetime(df_n.index)\n",
    "#     news_data = df_n['뉴스제목']\n",
    "#     news_x = senti_text_data(news_data,tokenizer)\n",
    "#     predicted_value = senti_model.predict(news_x)\n",
    "#     predicted_label = np.argmax(predicted_value, axis = 1)\n",
    "#     idx = np.where(predicted_label== senti)[::-1]\n",
    "#     df_senti = df_n.iloc[idx][:'2022-03-30'].sort_index(ascending=False)\n",
    "\n",
    "#     output_result = ''\n",
    "#     for i in range(len(df_senti)):\n",
    "#         title = df_senti['뉴스제목'].iloc[i]\n",
    "#         news_url = df_senti['url'].iloc[i]\n",
    "#         output_result += title + \"\\n\" + news_url + \"\\n\\n\"\n",
    "        \n",
    "#     return df_senti[:num], output_result\n",
    "\n",
    "# # 토크나이져 감정분류모델 불러오기\n",
    "# senti_model, senti_tokenizer = call_senti_model()\n",
    "\n",
    "# DB에서 정보추출\n",
    "def DB_info(name,db_type,date): #db_type: 종가,시가,PER,영업이익\n",
    "    end_point = \"chatbot-db.c9x08hbiunuu.ap-northeast-2.rds.amazonaws.com\"\n",
    "    port =3306\n",
    "    user_name = 'root'\n",
    "    pw ='123123123'\n",
    "\n",
    "    conn = pymysql.connect(\n",
    "        host = end_point,\n",
    "        user = user_name,\n",
    "        password = pw,\n",
    "    #     db = db,\n",
    "        charset='utf8'\n",
    "\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    sql = 'use Chatbot_DB;'\n",
    "    cursor.execute(sql)\n",
    "\n",
    "    sql = 'select * from stock_table;'\n",
    "\n",
    "    stock_table = pd.read_sql(sql, conn)\n",
    "    stock_df = stock_table[stock_table.name == name]\n",
    "    stock_df['날짜'] = stock_df['날짜'].astype('datetime64')\n",
    "\n",
    "    val = str(stock_df[stock_df['날짜'] == date][db_type].values).strip('[]').strip(\"''\")\n",
    "\n",
    "    if db_type == '영업이익':\n",
    "        print(f'{name}의 {db_type}은 {val}원 입니다')\n",
    "    \n",
    "    elif db_type == 'PER':\n",
    "        print(f'{name}의 {db_type}는 {val}배 입니다')\n",
    "\n",
    "    else: # 시가,종가\n",
    "        print(f'{name}의 {date} {db_type}는 {val}원 입니다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d63eab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "\n",
    " \n",
    "######## 텔레그램 관련 코드 ########\n",
    "token = \"5403110188:AAEbcgi6cDNmgdRHERhGhprFQgMUHzi-rtI\"\n",
    "id = 5322933876\n",
    " \n",
    "bot = telegram.Bot(token)\n",
    "# bot1 = TelegramBot(token,id)\n",
    "info_message = '''안녕하세요 금융챗봇 고슴도치입니다~'''\n",
    "bot.sendMessage(chat_id=id, text=info_message)\n",
    " \n",
    "updater = Updater(token=token, use_context=True)\n",
    "dispatcher = updater.dispatcher\n",
    "updater.start_polling()\n",
    " \n",
    " ### 챗봇 답장\n",
    "\n",
    "def handler(update, context): \n",
    "    user_text = update.message.text # 사용자가 보낸 메세지 user_text 변수에 저장\n",
    "    user_words = user_text.split()\n",
    "    \n",
    "    # 종목명 찾기\n",
    "    for word in user_words:\n",
    "        # 종목명을 포함한다면\n",
    "        if word in corp_list.tolist():\n",
    "            corp = [w for w in user_words if w in corp_list.tolist()][0]\n",
    "            break\n",
    "        else:\n",
    "            corp='종목명 없음'\n",
    "\n",
    "    # 종목명 없으면 gpt or 추천\n",
    "    if corp == '종목명 없음':\n",
    "        # 살만한 주식 뭐있어?\n",
    "        if user_text in reco_lst1:\n",
    "            bot.send_message(chat_id=id, text='\"에스엘\", \"태경케미컬\", \"흥아해운\" 종목입니다 \\n 30일 후 약 15% 상승 할 예정입니다.') # 답장 보내기\n",
    "\n",
    "        # 업종이 어떤거야?\n",
    "        elif user_text in reco_lst2:\n",
    "            bot.send_message(chat_id=id, text='섬유의복 업종 지수상승이 예상됩니다') # 답장 보내기\n",
    "\n",
    "        # 코스피 지수 or 주식시장\n",
    "        elif user_text in reco_lst3:\n",
    "            bot.send_message(chat_id=id, text='코스피지수가 하락할 예정입니다') # 답장 보내기\n",
    "            \n",
    "\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "        # gpt일반대화\n",
    "        else:\n",
    "            pass\n",
    "#             a = finance_gpt(user_text,tokenizer=koGPT2_TOKENIZER,model=model1)\n",
    "#             bot.send_message(chat_id=id, text=a) # 답장 보내기\n",
    "            \n",
    "    \n",
    "    # 종목명 있으면\n",
    "    else:\n",
    "#         for word in user_words:\n",
    "#             # 최신뉴스 최근뉴스 포함시\n",
    "#             if word in news_lst1:\n",
    "#                 _, recent_news = crawl_news(corp)\n",
    "#                 bot.send_message(chat_id=id, text= recent_news)\n",
    "#                 break\n",
    "#             # 긍정뉴스 호재있어??\n",
    "#             elif word in news_lst2:\n",
    "#                 _,pos_news = bert_clsfy_news(corp,senti_model=senti_model,tokenizer=senti_tokenizer,senti=1,page=5)\n",
    "#                 bot.send_message(chat_id=id, text= pos_news)\n",
    "#                 break\n",
    "#             # 부정뉴스\n",
    "#             elif word in news_lst3:\n",
    "#                 _,neg_text = bert_clsfy_news(corp,senti_model=senti_model,tokenizer=senti_tokenizer,senti=2,page=5)\n",
    "#                 bot.send_message(chat_id=id, text= neg_text)\n",
    "#                 break\n",
    "#             # 삼성전자 뭐랑관련있어? 최근 이슈? \n",
    "#             elif word in news_lst4:\n",
    "#                 kw_dict = kw_crawl_news(corp=corp)\n",
    "#                 keyword_list = list(kw_dict.keys())[:10]\n",
    "#                 kw_lst = []\n",
    "#                 for i in range(len(keyword_list)):\n",
    "#                     kw_lst.append('#'+keyword_list[i])\n",
    "#                 keyword_10 = ', '.join(kw_lst)\n",
    "#                 keyword_text = f'{corp}는 최근 {keyword_10} 과 관련이 있습니다'\n",
    "#                 plot = show_kw(kw_dict,num=15)\n",
    "#                 bot.send_message(chat_id=id, text= keyword_text)\n",
    "#                 bot1.send_plot(plot)\n",
    "#                 break\n",
    "        \n",
    "\n",
    "        if '어제' in user_words:\n",
    "            date = '2022-03-29'\n",
    "        else : \n",
    "            date = '2022-03-30'\n",
    "                \n",
    "        for word in user_words:\n",
    "            for nn in range(len(reco_lst4)):\n",
    "            # 삼성전자 얼마야?\n",
    "                if word == nn:\n",
    "                    val_text = DB_info('삼성전자', word, date)\n",
    "                    bot.send_message(chat_id=id, text=val_text) # 답장 보내기\n",
    "                    \n",
    "\n",
    "            \n",
    "        # 삼성전자 어떨꺼 같아?\n",
    "        if user_text in reco_lst5:  \n",
    "            bot.send_message(chat_id=id, text='삼성전자는 상승할 예정입니다.') # 답장 보내기\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "            # else:\n",
    "            #     bot.send_message(chat_id=id, text = '죄송해요 다시 한번 물어봐주세요')\n",
    "                \n",
    "\n",
    "echo_handler = MessageHandler(Filters.text, handler)\n",
    "dispatcher.add_handler(echo_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83df07f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09d5344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa46dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad233af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070df6c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d105330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dfe5aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a4d218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e680f623",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091e6c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b8df94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b3d96e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b4c972",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b395b7c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6d3801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7029fec3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9e2ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1552a459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf757421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d55186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabe747d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84befb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46788d66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815d5e10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da23cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35302b08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab5857a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bee455",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8ac25d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c228ab62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3109c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
