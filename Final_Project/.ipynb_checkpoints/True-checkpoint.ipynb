{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8adfba9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-telegram-bot in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (13.12)\n",
      "Requirement already satisfied: APScheduler==3.6.3 in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from python-telegram-bot) (3.6.3)\n",
      "Requirement already satisfied: cachetools==4.2.2 in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from python-telegram-bot) (4.2.2)\n",
      "Requirement already satisfied: pytz>=2018.6 in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from python-telegram-bot) (2021.3)\n",
      "Requirement already satisfied: certifi in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from python-telegram-bot) (2021.10.8)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from python-telegram-bot) (6.1)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from APScheduler==3.6.3->python-telegram-bot) (1.16.0)\n",
      "Requirement already satisfied: setuptools>=0.7 in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from APScheduler==3.6.3->python-telegram-bot) (58.0.4)\n",
      "Requirement already satisfied: tzlocal>=1.2 in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from APScheduler==3.6.3->python-telegram-bot) (4.2)\n",
      "Requirement already satisfied: tzdata in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from tzlocal>=1.2->APScheduler==3.6.3->python-telegram-bot) (2022.1)\n",
      "Requirement already satisfied: pytz-deprecation-shim in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from tzlocal>=1.2->APScheduler==3.6.3->python-telegram-bot) (0.1.0.post0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install python-telegram-bot --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3adbc39c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_addons\n",
      "  Downloading tensorflow_addons-0.17.1-cp39-cp39-win_amd64.whl (758 kB)\n",
      "     ------------------------------------- 758.1/758.1 kB 12.1 MB/s eta 0:00:00\n",
      "Collecting typeguard>=2.7\n",
      "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from tensorflow_addons) (21.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from packaging->tensorflow_addons) (3.0.4)\n",
      "Installing collected packages: typeguard, tensorflow_addons\n",
      "Successfully installed tensorflow_addons-0.17.1 typeguard-2.13.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d16b07d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting telegram\n",
      "  Downloading telegram-0.0.1.tar.gz (879 bytes)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: telegram\n",
      "  Building wheel for telegram (setup.py): started\n",
      "  Building wheel for telegram (setup.py): finished with status 'done'\n",
      "  Created wheel for telegram: filename=telegram-0.0.1-py3-none-any.whl size=1307 sha256=24bf57f09e13cef7bbd011a73393b96eb4d209d4f9df5cd2c9c1b77b425a2181\n",
      "  Stored in directory: c:\\users\\bitcamp\\appdata\\local\\pip\\cache\\wheels\\11\\7a\\5d\\62391dcb6b9a45247192c3a711bf03ed513c14218a8b275a63\n",
      "Successfully built telegram\n",
      "Installing collected packages: telegram\n",
      "Successfully installed telegram-0.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "# !pip install telegram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffe0dd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28e70195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import font_manager, rc\n",
    "font_path = \"C:/Windows/Fonts/malgun.ttf\"\n",
    "font = font_manager.FontProperties(fname=font_path).get_name()\n",
    "rc('font', family=font)\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88edcbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import telegram\n",
    "from telegram.ext import Updater\n",
    "from telegram.ext import MessageHandler, Filters\n",
    "from telegram import chat\n",
    "# from bob_telegram_tools.bot import TelegramBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "038a806f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9585982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import urllib.request\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38e27534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43f7a0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import MySQLdb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a615d6e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/GPT2_model/gpt_finance.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9880/1550217662.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mPAD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'<pad>'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mmodel1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/GPT2_model/gpt_finance.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m koGPT2_TOKENIZER = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n\u001b[0;32m     11\u001b[0m             \u001b[0mbos_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBOS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meos_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEOS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munk_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'<unk>'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    697\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/GPT2_model/gpt_finance.pth'"
     ]
    }
   ],
   "source": [
    "Q_TKN = \"<usr>\"\n",
    "A_TKN = \"<sys>\"\n",
    "BOS = '</s>'\n",
    "EOS = '</s>'\n",
    "MASK = '<unused0>'\n",
    "SENT = '<unused1>'\n",
    "PAD = '<pad>'\n",
    "\n",
    "model1 = torch.load('./data/GPT2_model/gpt_finance.pth')\n",
    "koGPT2_TOKENIZER = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
    "            bos_token=BOS, eos_token=EOS, unk_token='<unk>',\n",
    "            pad_token=PAD, mask_token=MASK) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba699c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "kospi_list = pd.read_csv('./data/recent_kospi_list.csv')\n",
    "corp_list = kospi_list['Name']\n",
    "news_lst1 = ['최신뉴스','최근뉴스']\n",
    "news_lst2 = ['긍정뉴스', '호재', '좋은소식','호재는']\n",
    "news_lst3 = ['부정뉴스', '악재','악재는?']\n",
    "news_lst4 = ['키워드','관련있어','관련있어?']\n",
    "\n",
    "\n",
    "reco_lst1 = ['살만한', '주식', '뭐있어','종목', '뭐','있어', '뭐야','투자', '종목추천']\n",
    "reco_lst2 = ['어떤', '업종이', '괜찮아', '업종','업종추천']\n",
    "reco_lst3 = ['코스피','주식시장']\n",
    "reco_lst4 = ['시가','종가','영업이익','PER']\n",
    "reco_lst5 = ['어떨꺼', '같아','어때','보여','오를까', '내릴까']\n",
    "\n",
    " ######## 함수 ########\n",
    "\n",
    "def crawl_news(corp,page=1,num=5,bgn_date='2022.03.01',end_date='2022.03.30'):\n",
    "    \n",
    "    bgn_date1 = bgn_date\n",
    "    bgn_date2 = bgn_date.replace('.','')\n",
    "    end_date1 = end_date\n",
    "    end_date2 = end_date.replace('.','')\n",
    "    \n",
    "    title_lst = []\n",
    "    url_lst = []\n",
    "    date_lst = []\n",
    "\n",
    "    for pg in range(1,page+1):\n",
    "\n",
    "        page_num = pg *10 - 9\n",
    "\n",
    "        url = f'https://search.naver.com/search.naver?where=news&sm=tab_pge&query={corp}&sort=0&photo=0&field=0&pd=3&ds={bgn_date1}&de={end_date1}&cluster_rank=24&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:r,p:from{bgn_date2}to{end_date2},a:all&start={page_num}'\n",
    "        res = requests.get(url)\n",
    "        soup = BeautifulSoup(res.text , 'html.parser')\n",
    "        lis = soup.select('#main_pack > section > div > div.group_news > ul>li')\n",
    "\n",
    "        for li in lis:\n",
    "            #제목\n",
    "            title = li.select('div.news_wrap.api_ani_send > div > a')[0].text\n",
    "\n",
    "            title_lst.append(title)\n",
    "\n",
    "            # url\n",
    "            url_path = li.select('div.news_wrap.api_ani_send > div > a')[0]['href']\n",
    "            url_lst.append(url_path)\n",
    "\n",
    "            #날짜\n",
    "\n",
    "            if len(li.select('div.news_info > div.info_group > span'))==1:\n",
    "                date = li.select('div.news_info > div.info_group > span')[0].text\n",
    "                date_lst.append(date)\n",
    "\n",
    "\n",
    "            if len(li.select('div.news_info > div.info_group > span'))==2:\n",
    "                date = li.select('div.news_info > div.info_group > span')[1].text\n",
    "                date_lst.append(date)\n",
    "    \n",
    "    df = pd.DataFrame({'날짜':date_lst,'뉴스제목':title_lst,'url':url_lst})\n",
    "    \n",
    "    output_result = ''\n",
    "    for i in range(len(df)):\n",
    "        title = df['뉴스제목'].iloc[i]\n",
    "        news_url = df['url'].iloc[i]\n",
    "        output_result += title + \"\\n\" + news_url + \"\\n\\n\"\n",
    "        if i == num:\n",
    "            break\n",
    "        \n",
    "    return df, output_result\n",
    "\n",
    "def finance_gpt(user_text,tokenizer, model):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        answer = \"\"\n",
    "        while 1:\n",
    "            input_ids = torch.LongTensor(tokenizer.encode(Q_TKN + user_text + SENT + A_TKN + answer)).unsqueeze(dim=0)\n",
    "            pred = model(input_ids)\n",
    "            pred = pred.logits\n",
    "            gen = tokenizer.convert_ids_to_tokens(torch.argmax(pred, dim=-1).squeeze().numpy().tolist())[-1]\n",
    "            if gen == EOS:\n",
    "                break\n",
    "            answer += gen.replace(\"▁\", \" \")\n",
    "        answer = answer.strip()\n",
    "    return answer\n",
    "\n",
    "# 한글,영어만 남기기\n",
    "def clean_text(docs):\n",
    "    docs = re.sub('[^가-힣A-Za-z ]', '', str(docs))\n",
    "    docs = re.sub('\\s+', ' ', docs)\n",
    "    docs = '' if docs== ' ' else docs\n",
    "    return docs\n",
    "\n",
    "# 명사 추출\n",
    "def han_noun(docs):\n",
    "    han = Hannanum()\n",
    "    docs = han.nouns(docs)\n",
    "    return docs\n",
    "\n",
    "# 불용어 제거+ 한글자 이상만 남기기\n",
    "stw_list = pd.read_csv('./data/stopwords-ko.txt')\n",
    "def remove_stwords(docs):\n",
    "    docs = [w for w in docs if not w in stw_list]\n",
    "    docs = '' if docs== ' ' else docs\n",
    "    docs = [w for w in docs if len(w)>1]\n",
    "    return docs\n",
    "\n",
    "# 크롤링 뉴스 키워드 선출\n",
    "def kw_crawl_news(corp,bgn_date='2022-03-01',end_date='2022-03-30'):\n",
    "    df_n, _ = crawl_news(corp,page=10)\n",
    "    df_n = df_n.set_index('날짜')\n",
    "    df_n.index = pd.to_datetime(df_n.index)\n",
    "    df_n['뉴스'] = df_n['뉴스제목'].apply(clean_text)\n",
    "    df_n['뉴스'] = df_n['뉴스'].apply(han_noun)\n",
    "    df_n['뉴스'] =  df_n['뉴스'].apply(remove_stwords)\n",
    "    df_stw = pd.read_csv('./data/뉴스불용어2.csv',index_col=1)\n",
    "    stw_lst = df_stw['불용어'].tolist()\n",
    "    kw_dict = dict()\n",
    "\n",
    "    token_lst = df_n['뉴스'].loc[bgn_date:end_date]\n",
    "            \n",
    "    for tokens in token_lst:\n",
    "        for word in tokens:\n",
    "                \n",
    "            if not word in kw_dict.keys():\n",
    "                kw_dict[word] = 1\n",
    "            else:\n",
    "                kw_dict[word] += 1\n",
    "\n",
    "    kw_dict = dict(sorted(kw_dict.items(), key = lambda x: x[1],reverse=True))\n",
    "    keys = pd.Series(kw_dict.keys()).tolist()\n",
    "    for key in keys:\n",
    "        if (key in stw_lst) | (key == corp):\n",
    "            del kw_dict[key]\n",
    "            \n",
    "    return kw_dict\n",
    "\n",
    "# 키워드 시각화\n",
    "def show_kw(kw_dict,num=10):\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.rc('ytick', labelsize=15)  # y축 눈금 폰트 크기\n",
    "    plt.title('키워드 상위 15')\n",
    "    keyword_list = list(kw_dict.keys())[:num]\n",
    "    keyword_list.reverse()\n",
    "    keyword_count = list(kw_dict.values())[:num]\n",
    "    keyword_count.reverse()\n",
    "    sns.barplot(x=keyword_count, y=keyword_list)\n",
    "    plt.gca().invert_yaxis()\n",
    "    return plt\n",
    "\n",
    "# 최고 성능의 모델 불러오기\n",
    "def call_senti_model():\n",
    "    BEST_MODEL_NAME = './data/model/best_model.h5'\n",
    "    sentiment_model_best = tf.keras.models.load_model(BEST_MODEL_NAME,\n",
    "                                                      custom_objects={'TFBertForSequenceClassification': TFBertForSequenceClassification})\n",
    "    MODEL_NAME = \"klue/bert-base\"\n",
    "    model = TFBertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3, from_pt=True)\n",
    "    tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "    return sentiment_model_best, tokenizer\n",
    "\n",
    "# 데이터형태 맞춰주기\n",
    "def senti_text_data(X_data,tokenizer,MAX_SEQ_LEN=64):\n",
    "    # BERT 입력으로 들어가는 token, mask, segment, target 저장용 리스트\n",
    "    tokens, masks, segments, targets = [], [], [], []\n",
    "    \n",
    "    for X in tqdm(X_data):\n",
    "        # token: 입력 문장 토큰화\n",
    "        token = tokenizer.encode(X, truncation = True, padding = 'max_length', max_length = MAX_SEQ_LEN)\n",
    "        \n",
    "        # Mask: 토큰화한 문장 내 패딩이 아닌 경우 1, 패딩인 경우 0으로 초기화\n",
    "        num_zeros = token.count(0)\n",
    "        mask = [1] * (MAX_SEQ_LEN - num_zeros) + [0] * num_zeros\n",
    "        \n",
    "        # segment: 문장 전후관계 구분: 오직 한 문장이므로 모두 0으로 초기화\n",
    "        segment = [0]*MAX_SEQ_LEN\n",
    "\n",
    "        tokens.append(token)\n",
    "        masks.append(mask)\n",
    "        segments.append(segment)\n",
    "\n",
    "\n",
    "    # numpy array로 저장\n",
    "    tokens = np.array(tokens)\n",
    "    masks = np.array(masks)\n",
    "    segments = np.array(segments)\n",
    "    return [tokens, masks, segments]\n",
    "\n",
    "# BERT 긍부정 판별\n",
    "def bert_clsfy_news(corp,senti_model,tokenizer,senti=1,page=5,num=5): #1 2 0 긍정 부정 중립\n",
    "    df_n,_ = crawl_news(corp,page)\n",
    "    df_n = df_n.set_index('날짜')\n",
    "    df_n.index = pd.to_datetime(df_n.index)\n",
    "    news_data = df_n['뉴스제목']\n",
    "    news_x = senti_text_data(news_data,tokenizer)\n",
    "    predicted_value = senti_model.predict(news_x)\n",
    "    predicted_label = np.argmax(predicted_value, axis = 1)\n",
    "    idx = np.where(predicted_label== senti)[::-1]\n",
    "    df_senti = df_n.iloc[idx][:'2022-03-30'].sort_index(ascending=False)\n",
    "\n",
    "    output_result = ''\n",
    "    for i in range(len(df_senti)):\n",
    "        title = df_senti['뉴스제목'].iloc[i]\n",
    "        news_url = df_senti['url'].iloc[i]\n",
    "        output_result += title + \"\\n\" + news_url + \"\\n\\n\"\n",
    "        \n",
    "    return df_senti[:num], output_result\n",
    "\n",
    "# 토크나이져 감정분류모델 불러오기\n",
    "senti_model, senti_tokenizer = call_senti_model()\n",
    "\n",
    "# DB에서 정보추출\n",
    "def DB_info(name,db_type,date): #db_type: 종가,시가,PER,영업이익\n",
    "    end_point = \"chatbot-db.c9x08hbiunuu.ap-northeast-2.rds.amazonaws.com\"\n",
    "    port =3306\n",
    "    user_name = 'root'\n",
    "    pw ='123123123'\n",
    "\n",
    "    conn = pymysql.connect(\n",
    "        host = end_point,\n",
    "        user = user_name,\n",
    "        password = pw,\n",
    "    #     db = db,\n",
    "        charset='utf8'\n",
    "\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    sql = 'use Chatbot_DB;'\n",
    "    cursor.execute(sql)\n",
    "\n",
    "    sql = 'select * from stock_table;'\n",
    "\n",
    "    stock_table = pd.read_sql(sql, conn)\n",
    "    stock_df = stock_table[stock_table.name == name]\n",
    "    stock_df['날짜'] = stock_df['날짜'].astype('datetime64')\n",
    "\n",
    "    val = str(stock_df[stock_df['날짜'] == date][db_type].values).strip('[]').strip(\"''\")\n",
    "\n",
    "    if db_type == '영업이익':\n",
    "        print(f'{name}의 {db_type}은 {val}원 입니다')\n",
    "    \n",
    "    elif db_type == 'PER':\n",
    "        print(f'{name}의 {db_type}는 {val}배 입니다')\n",
    "\n",
    "    else: # 시가,종가\n",
    "        print(f'{name}의 {date} {db_type}는 {val}원 입니다')\n",
    "        \n",
    "# 종목 추세         \n",
    "def companyy_predict(name, date):\n",
    "    \n",
    "    end_point = \"chatbot-db.c9x08hbiunuu.ap-northeast-2.rds.amazonaws.com\"\n",
    "    port =3306\n",
    "    user_name = 'root'\n",
    "    pw ='123123123'\n",
    "    \n",
    "    conn = pymysql.connect(\n",
    "    host = end_point,\n",
    "    user = user_name,\n",
    "    password = pw,\n",
    "#     db = db,\n",
    "    charset='utf8'\n",
    "    \n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    sql = 'use Name_Code_DB;'\n",
    "    cursor.execute(sql)\n",
    "    \n",
    "    sql = 'select * from name_table;'\n",
    "\n",
    "    name_df = pd.read_sql(sql, conn)\n",
    "    \n",
    "    for na in range(len(name_df)):\n",
    "        if name == name_df.iloc[na,1]:\n",
    "            code = name_df.iloc[na,0]\n",
    "    \n",
    "    autoencoder = load_model(f'c:/Users/bitcamp/Desktop/final_data/autoencoder/Conv_2/auto_conv_{code}.h5')\n",
    "\n",
    "    company_df = fdr.DataReader(code)\n",
    "    stock_close = company_df[['Close']]\n",
    "\n",
    "    stock_close.columns = ['price']\n",
    "    stock_close['pct_change'] = stock_close.price.pct_change()\n",
    "    stock_close['log_ret'] = np.log(com_close.price) - np.log(stock_close.price.shift(1))\n",
    "    stock_close.dropna(inplace=True)\n",
    "\n",
    "    x_test = stock_close[stock_close.index == date]\n",
    "\n",
    "    window_length = 10\n",
    "    scaler = MinMaxScaler()\n",
    "    x_test_nonscaled = np.array([stock_close['log_ret'].values[i-window_length:i].reshape(-1, 1) for i in tqdm(range(window_length+1,len(stock_close['log_ret'])))])\n",
    "    x_test = np.array([scaler.fit_transform(stock_close['log_ret'].values[i-window_length:i].reshape(-1, 1)) for i in tqdm(range(window_length+1,len(stock_close['log_ret'])))])\n",
    "\n",
    "    y_predict = autoencoder.predict(x_test)\n",
    "    labels = [1,0]\n",
    "\n",
    "    label = labels[y_predict[0].argmax()]\n",
    "    confidence = y_predict[0][y_predict[0].argmax()]\n",
    "    # print('{} {:.2f}%'.format(label, confidence * 100))\n",
    "\n",
    "    if label == 1:\n",
    "        print(f'{name}는 상승할 예정입니다.')\n",
    "    else:\n",
    "        print(f'{name}는 하락할 예정입니다.')\n",
    "        \n",
    "\n",
    "# 종목별 AI 추천\n",
    "def company_recomend(date):\n",
    "    end_point = \"chatbot-db.c9x08hbiunuu.ap-northeast-2.rds.amazonaws.com\"\n",
    "    port =3306\n",
    "    user_name = 'root'\n",
    "    pw ='123123123'\n",
    "\n",
    "    conn = pymysql.connect(\n",
    "        host = end_point,\n",
    "        user = user_name,\n",
    "        password = pw,\n",
    "    #     db = db,\n",
    "        charset='utf8'\n",
    "\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    sql = 'use Chatbot_DB;'\n",
    "    cursor.execute(sql)\n",
    "\n",
    "    sql = 'select * from stock_table;'\n",
    "\n",
    "    stock_table = pd.read_sql(sql, conn)\n",
    "    \n",
    "    \n",
    "    stock_table['날짜'] = stock_table['날짜'].astype('datetime64')\n",
    "    stock_df = stock_table[stock_table['날짜'] == date]\n",
    "    stock_df = stock_df.drop(['BPS', 'PER', 'PBR', 'EPS', 'DIV_', 'DPS', 'quarter','날짜','등락률','name'], axis=1)\n",
    "    stock_df = stock_df.astype(str).apply(lambda col : col.apply(lambda x: np.nan if x == 'nan' else x))\n",
    "    stock_df.dropna(inplace=True)\n",
    "\n",
    "    hoho = defaultdict(list)\n",
    "    plus_df = pd.DataFrame()\n",
    "    hoit_df = pd.DataFrame()\n",
    "    fail_li = []\n",
    "    code_li = []\n",
    "    month_li = ['30일','3개월','6개월']\n",
    "    per_li = ['5per','10per','15per']\n",
    "\n",
    "    for code in stock_df['code'][:]:\n",
    "        try:\n",
    "\n",
    "            df = pd.read_csv(f'c:/Users/bitcamp/Desktop/final_data/머신러닝_재무데이터/머신러닝_train_test/{month}/test/{code}.csv', index_col=0)\n",
    "            df.drop(['date','등락률','종목명','수익률','quarter','5per','10per','15per'], axis=1, inplace=True)\n",
    "            df.rename(columns={'법인세차감전 순이익':'법인세차감전_순이익'}, inplace=True)\n",
    "\n",
    "            ya_df = pd.concat([stock_df[['code']],stock_df[list(df.columns)]], axis=1)\n",
    "            x_test = ya_df[ya_df.code == code].iloc[0,1:]\n",
    "\n",
    "            for monmon in month_li[:]:\n",
    "                for per in per_li[:]:\n",
    "\n",
    "                        with open(f'c:/Users/bitcamp/Desktop/final_data/머신러닝_모델/{monmon}_{per}/Randomforest/{code}.pkl','rb') as f:\n",
    "                                rfc = pickle.load(f)\n",
    "\n",
    "                        pred = rfc.predict([x_test])\n",
    "\n",
    "                        col = monmon+'_'+per\n",
    "\n",
    "                        hoho[col].append(pred[0])\n",
    "            code_li.append(code)\n",
    "\n",
    "        except:\n",
    "#             print(code)\n",
    "            pass\n",
    "\n",
    "    plus_df = pd.DataFrame({'code':code_li})\n",
    "\n",
    "    for col,val in hoho.items():\n",
    "        plus_df[col] = val\n",
    "    \n",
    "#     print(plus_df.head())\n",
    "    \n",
    "    max_plus_df = plus_df[plus_df['30일_15per']== 1]\n",
    "    max_plus_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    pre_df = pd.read_csv('c:/Users/bitcamp/Desktop/final_data/모델정리/종목별_머신러닝/시연용/rfc_30_15.csv', index_col=0)\n",
    "    pre_df.code = pre_df.code.apply(lambda x:f'{x:06}')\n",
    "\n",
    "    company_li = []\n",
    "    for pre in range(len(pre_df)):\n",
    "        for coco in range(len(max_plus_df)):\n",
    "            if max_plus_df.iloc[coco, 0] == pre_df.iloc[pre, 0]:\n",
    "                company_li.append(pre_df.iloc[pre,1])\n",
    "\n",
    "    return company_li[1].strip('[]').strip(\"''\"), company_li[2].strip('[]').strip(\"''\"), company_li[3].strip('[]').strip(\"''\")\n",
    "\n",
    "# 코스피 업종 추천\n",
    "def kospi_kind_recomend():\n",
    "    kospi_df = pd.read_csv('c:/Users/bitcamp/Desktop/final_data/업종별/월봉_21_8.csv')\n",
    "\n",
    "    kospi_corr_df = kospi_df.corr()\n",
    "    kospi_corr_df = kospi_corr_df[['코스피_5']]\n",
    "\n",
    "    kospi_corr_df.reset_index(drop=False, inplace=True)\n",
    "    kospi_corr_df.columns = ['업종', 'kospi']\n",
    "\n",
    "    # kospi_corr_df.sort_values(by=['kospi'], ascending=False)\n",
    "    kospi_kind_df = kospi_corr_df.sort_values(by=['kospi'])\n",
    "    kind_recomend = kospi_kind_df.iloc[0,0]\n",
    "    return kind_recomend.split('_')[0]\n",
    "\n",
    "# 코스피 추세 예측\n",
    "def kospi_predict(date):\n",
    "    kospi = fdr.DataReader('KS11')\n",
    "    kospi_df = kospi[['Close']]\n",
    "\n",
    "    stock = pd.DataFrame()\n",
    "    stock['y'] = kospi_df[['Close']]\n",
    "    stock['ds'] = kospi_df.index\n",
    "\n",
    "    stock.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    train = stock[stock.ds < '2021-01']\n",
    "    test = stock[stock.ds > '2021-01']\n",
    "\n",
    "    df_prophet = Prophet(changepoint_prior_scale=0.15, daily_seasonality=True)\n",
    "    df_prophet.fit(train)\n",
    "\n",
    "    fcast_time = 100\n",
    "    df_forecast = df_prophet.make_future_dataframe(periods=fcast_time, freq='D')\n",
    "    # df_forecast.tail(50)\n",
    "    df_forecast = df_prophet.predict(df_forecast)\n",
    "\n",
    "    kospi_updow_df = df_forecast[['ds','yhat','yhat_lower','yhat_upper']]\n",
    "#     kospi_updow_df['ds'] = kospi_updow_df['ds'].astype('datetime64')\n",
    "\n",
    "    kospi_1 = str(kospi_updow_df[kospi_updow_df['ds'] == date]['yhat_upper'].values)\n",
    "    kospi_1 = kospi_1.strip('[]')[:4]\n",
    "\n",
    "    kospi_2 = str(kospi_updow_df[kospi_updow_df['ds'] == date]['yhat_upper'].values)\n",
    "    kospi_2 = kospi_2.strip('[]')[:4]\n",
    "\n",
    "    n = int(kospi_2) - int(kospi_1)\n",
    "\n",
    "    if n > 0:\n",
    "        print('코스피지수가 상승할 예정입니다.')\n",
    "    else :\n",
    "        print('코스피지수가 하락할 예정입니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38091fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "\n",
    " \n",
    "######## 텔레그램 관련 코드 ########\n",
    "token = \"5403110188:AAEbcgi6cDNmgdRHERhGhprFQgMUHzi-rtI\"\n",
    "id = 5322933876\n",
    " \n",
    "bot = telegram.Bot(token)\n",
    "# bot1 = TelegramBot(token,id)\n",
    "info_message = '''안녕하세요 금융챗봇 고슴도치입니다~'''\n",
    "bot.sendMessage(chat_id=id, text=info_message)\n",
    " \n",
    "updater = Updater(token=token, use_context=True)\n",
    "dispatcher = updater.dispatcher\n",
    "updater.start_polling()\n",
    "\n",
    "if '어제' in user_words:\n",
    "    date = '2022-03-29'\n",
    "else : \n",
    "    date = '2022-03-30'\n",
    "\n",
    "for com in user_words:\n",
    "    if com in kospi_list:\n",
    "        company = com\n",
    "    else:\n",
    "        bot.send_message(chat_id=id, text='회사명을 다시 써주세요')\n",
    " \n",
    " ### 챗봇 답장\n",
    "\n",
    "def handler(update, context): \n",
    "    user_text = update.message.text # 사용자가 보낸 메세지 user_text 변수에 저장\n",
    "    user_words = user_text.split()\n",
    "    \n",
    "    # 종목명 찾기\n",
    "    for word in user_words:\n",
    "        # 종목명을 포함한다면\n",
    "        if word in corp_list.tolist():\n",
    "            corp = [w for w in user_words if w in corp_list.tolist()][0]\n",
    "            break\n",
    "        else:\n",
    "            corp='종목명 없음'\n",
    "\n",
    "    # 종목명 없으면 gpt or 추천\n",
    "    if corp == '종목명 없음':\n",
    "        # 살만한 주식 뭐있어?\n",
    "        if user_words in reco_lst1:\n",
    "            val_text = company_recomend(date)\n",
    "            bot.send_message(chat_id=id, text=val_text) # 답장 보내기\n",
    "\n",
    "        # 업종이 어떤거야?\n",
    "        elif user_words in reco_lst2:\n",
    "            val_text = kospi_kind_recomend()\n",
    "            bot.send_message(chat_id=id, text=val_text) # 답장 보내기\n",
    "\n",
    "        # 코스피 지수 or 주식시장\n",
    "        elif user_words in reco_lst3:\n",
    "            val_text = kospi_predict(date)\n",
    "            bot.send_message(chat_id=id, text=val_text) # 답장 보내기\n",
    "            \n",
    "\n",
    "           \n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "        # gpt일반대화\n",
    "        else:\n",
    "#             pass\n",
    "            a = finance_gpt(user_text,tokenizer=koGPT2_TOKENIZER,model=model1)\n",
    "            bot.send_message(chat_id=id, text=a) # 답장 보내기\n",
    "            \n",
    "    \n",
    "    # 종목명 있으면\n",
    "    else:\n",
    "        for word in user_words:\n",
    "            # 최신뉴스 최근뉴스 포함시\n",
    "            if word in news_lst1:\n",
    "                _, recent_news = crawl_news(corp)\n",
    "                bot.send_message(chat_id=id, text= recent_news)\n",
    "                break\n",
    "            # 긍정뉴스 호재있어??\n",
    "            elif word in news_lst2:\n",
    "                _,pos_news = bert_clsfy_news(corp,senti_model=senti_model,tokenizer=senti_tokenizer,senti=1,page=5)\n",
    "                bot.send_message(chat_id=id, text= pos_news)\n",
    "                break\n",
    "            # 부정뉴스\n",
    "            elif word in news_lst3:\n",
    "                _,neg_text = bert_clsfy_news(corp,senti_model=senti_model,tokenizer=senti_tokenizer,senti=2,page=5)\n",
    "                bot.send_message(chat_id=id, text= neg_text)\n",
    "                break\n",
    "            # 삼성전자 뭐랑관련있어? 최근 이슈? \n",
    "            elif word in news_lst4:\n",
    "                kw_dict = kw_crawl_news(corp=corp)\n",
    "                keyword_list = list(kw_dict.keys())[:10]\n",
    "                kw_lst = []\n",
    "                for i in range(len(keyword_list)):\n",
    "                    kw_lst.append('#'+keyword_list[i])\n",
    "                keyword_10 = ', '.join(kw_lst)\n",
    "                keyword_text = f'{corp}는 최근 {keyword_10} 과 관련이 있습니다'\n",
    "                plot = show_kw(kw_dict,num=15)\n",
    "                bot.send_message(chat_id=id, text= keyword_text)\n",
    "                bot1.send_plot(plot)\n",
    "                break\n",
    "        \n",
    "\n",
    "                \n",
    "        # 삼성전자 얼마야?\n",
    "        for word in range(len(user_words)):\n",
    "            if user_words[word] in reco_lst4:\n",
    "                val_text = DB_info(company,user_words[word], date)\n",
    "                bot.send_message(chat_id=id, text=val_text) # 답장 보내기\n",
    "            \n",
    "            \n",
    "        # 삼성전자 어떨꺼 같아?\n",
    "        if user_words in reco_lst5:\n",
    "            val_text = company_predict(company,date)\n",
    "            bot.send_message(chat_id=id, text=val_text) # 답장 보내기\n",
    "            \n",
    "            \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "            # else:\n",
    "            #     bot.send_message(chat_id=id, text = '죄송해요 다시 한번 물어봐주세요')\n",
    "                \n",
    "\n",
    "echo_handler = MessageHandler(Filters.text, handler)\n",
    "dispatcher.add_handler(echo_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b5e5e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba391639",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8398d2e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a378f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d9f87b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d8f07c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbe8682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd1e927",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9116e27b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f04dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfb8ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8094458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46c335f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236dc0f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7352b64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da0cbdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b55c55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4674676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25af46ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea9b2f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f1d2fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c619be0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fdf51a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042298f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dc39a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1795cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27132a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f242be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c9b3ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f849f110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81ae71f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
